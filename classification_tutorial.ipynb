{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python machine learning classification tutorial\n",
    "This tutorial explains basic concepts neccesary for using sci-kit learn's classification tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Set-up\n",
    "The `CountVectorizer` class implements both tokenization and occurence counting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "texts = [u'Hello I\\'m a text and I am the first text in this collection of texts',\n",
    "         u'And I am the second one. I like olives.',\n",
    "         u'I hate olives. I am the grumpy third text.']\n",
    "\n",
    "vect = CountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Fit and transform the data\n",
    "The `fit_transform` method is a combination of the `fit` and `transform` methods. The method mutates the vector and returns a matrix representation of the tokens.\n",
    "\n",
    "* `fit`: Learn a vocabulary dictionary of all tokens in the raw documents.\n",
    "* `transform`: Transform documents to document-term matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = vect.fit_transform(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can call `toarray()` on the matrix to better understand how the tokens are saved. Each \"row\" contains as many elements as there are unique words in the data, and the presence of each word is indicated with a 1, the abscence with a 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 2, 1, 1, 0, 1],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0],\n",
       "       [1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3. Vector methods after fit_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary\n",
    "The count vector has a dict – `vocabulary_` – that maps tokens to their integer representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'and': 1, u'text': 13, u'am': 0, u'collection': 2, u'one': 11, u'texts': 14, u'second': 12, u'in': 7, u'hate': 5, u'the': 15, u'olives': 10, u'like': 8, u'third': 16, u'this': 17, u'of': 9, u'grumpy': 4, u'hello': 6, u'first': 3}\n"
     ]
    }
   ],
   "source": [
    "print vect.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature names\n",
    "The `get_feature_names` method returns an ordered list of all the unique tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'am', u'and', u'collection', u'first', u'grumpy', u'hate', u'hello', u'in', u'like', u'of', u'olives', u'one', u'second', u'text', u'texts', u'the', u'third', u'this']\n"
     ]
    }
   ],
   "source": [
    "print vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compute frequencies\n",
    "In order to compute the relative frequency of the tokens in each document, we make use of a computation called _Term frequency times inverse document frequency_. It basically calculates the frequency for each word in every document, but adjusts that frequency based on how common the word is among all documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "X_tfidf = TfidfTransformer().fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train a classifier\n",
    "Note: Abandoning previous train of thought, and trying out a classifier on real data head on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_texts = []\n",
    "political_texts = {'fi': 'https://snd.gu.se/vivill/file/43/txt',\n",
    "                    'c': 'https://snd.gu.se/vivill/file/41/txt',\n",
    "                    'kd': 'https://snd.gu.se/vivill/file/94/txt',\n",
    "                    'mp': 'https://snd.gu.se/vivill/file/153/txt',\n",
    "                    'm': 'https://snd.gu.se/vivill/file/144/txt',\n",
    "                    'sd': 'https://snd.gu.se/vivill/file/202/txt',\n",
    "                    's': 'https://snd.gu.se/vivill/file/200/txt',\n",
    "                    'v': 'https://snd.gu.se/vivill/file/238/txt'\n",
    "                   }\n",
    "\n",
    "for party, url in political_texts.iteritems():\n",
    "    r = requests.get(url)\n",
    "    training_texts.append({'party': party, 'text': r.text})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the term matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vector = CountVectorizer()\n",
    "X = vector.fit_transform([x['text'] for x in training_texts])\n",
    "\n",
    "labels = [x['party'] for x in training_texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the classifier and fit the data and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = MultinomialNB().fit(X, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(docs):\n",
    "    \"\"\"Takes a document collection and returns the predicted party\"\"\"\n",
    "    X = vector.transform(docs)\n",
    "    return clf.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docs_new = [u'Jag tror på en feministisk och solidarisk politik.',\n",
    "            u'Vi vill ha ett fritt samhälle utan tvång.',\n",
    "            u'Vi måste våga sätta gränser för invandringen och ta hand om våra egna äldre.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Jag tror på en feministisk och solidarisk politik.\" is probably from ==> V\n",
      "\"Vi vill ha ett fritt samhälle utan tvång.\" is probably from ==> M\n",
      "\"Vi måste våga sätta gränser för invandringen och ta hand om våra egna äldre.\" is probably from ==> M\n"
     ]
    }
   ],
   "source": [
    "for doc, category in zip(docs_new, predict(docs_new)):\n",
    "    print '\"%s\" is probably from ==> %s' % (doc, category.upper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
