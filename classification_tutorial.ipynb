{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python machine learning classification tutorial\n",
    "This tutorial explains basic concepts neccesary for using sci-kit learn's classification tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Set-up\n",
    "The `CountVectorizer` class implements both tokenization and occurence counting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "texts = [u'Hello I\\'m a text and I am the first text in this collection of texts',\n",
    "         u'And I am the second one. I like olives.',\n",
    "         u'I hate olives. I am the grumpy third text.']\n",
    "\n",
    "vect = CountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Fit and transform the data\n",
    "The `fit_transform` method is a combination of the `fit` and `transform` methods. The method mutates the vector and returns a matrix representation of the tokens.\n",
    "\n",
    "* `fit`: Learn a vocabulary dictionary of all tokens in the raw documents.\n",
    "* `transform`: Transform documents to document-term matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = vect.fit_transform(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can call `toarray()` on the matrix to better understand how the tokens are saved. Each \"row\" contains as many elements as there are unique words in the data, and the presence of each word is indicated with a 1, the abscence with a 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 2, 1, 1, 0, 1],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0],\n",
       "       [1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3. Vector methods after fit_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary\n",
    "The count vector has a dict – `vocabulary_` – that maps tokens to their integer representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'and': 1, u'text': 13, u'am': 0, u'collection': 2, u'one': 11, u'texts': 14, u'second': 12, u'in': 7, u'hate': 5, u'the': 15, u'olives': 10, u'like': 8, u'third': 16, u'this': 17, u'of': 9, u'grumpy': 4, u'hello': 6, u'first': 3}\n"
     ]
    }
   ],
   "source": [
    "print vect.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature names\n",
    "The `get_feature_names` method returns an ordered list of all the unique tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'am', u'and', u'collection', u'first', u'grumpy', u'hate', u'hello', u'in', u'like', u'of', u'olives', u'one', u'second', u'text', u'texts', u'the', u'third', u'this']\n"
     ]
    }
   ],
   "source": [
    "print vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compute frequencies\n",
    "In order to compute the relative frequency of the tokens in each document, we make use of a computation called _Term frequency times inverse document frequency_. It basically calculates the frequency for each word in every document, but adjusts that frequency based on how common the word is among all documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "X_tfidf = TfidfTransformer().fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. T"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
